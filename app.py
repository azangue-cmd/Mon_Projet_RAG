import os
import tempfile

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. Configuration de la page
st.set_page_config(page_title="Chat avec tes PDF (Local & Priv√©)", page_icon="üîí")
st.title("üîí RAG Cybers√©curit√© : Analyse de PDF en Local")
st.markdown("Ce projet utilise **Ollama (Mistral)** pour analyser des documents sensibles sans connexion internet.")

# 2. Fonction pour traiter le PDF
def process_pdf(uploaded_file):
    # Cr√©ation d'un fichier temporaire pour que PyPDFLoader puisse le lire
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    # Chargement du PDF
    loader = PyPDFLoader(tmp_path)
    docs = loader.load()

    # D√©coupage du texte en morceaux (Chunks)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # Cr√©ation de la base de donn√©es vectorielle (Embeddings)
    # On utilise Ollama pour transformer le texte en vecteurs math√©matiques
    embeddings = OllamaEmbeddings(model="mistral")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    
    # Nettoyage du fichier temporaire
    os.remove(tmp_path)
    return vectorstore

# 3. Interface Utilisateur
uploaded_file = st.file_uploader("D√©pose ton document PDF ici", type="pdf")

if uploaded_file is not None:
    st.success("Fichier charg√© ! Traitement en cours...")
    
    # Cr√©ation de la base de connaissance
    if "vectorstore" not in st.session_state:
        with st.spinner("Indexation du document... (Cela peut prendre un peu de temps sur CPU)"):
            st.session_state.vectorstore = process_pdf(uploaded_file)
            st.success("Document index√© ! Tu peux poser tes questions.")

    # Zone de Chat
    question = st.text_input("Pose ta question sur le document :")

    if question:
        # 4. La partie RAG (R√©cup√©ration + G√©n√©ration)
        
        # Le mod√®le va chercher les morceaux pertinents dans le PDF
        retriever = st.session_state.vectorstore.as_retriever()
        
        # Le mod√®le de langage (LLM)
        llm = Ollama(model="mistral")
        
        # Le Prompt (Les instructions donn√©es √† l'IA)
        template = """Tu es un assistant expert en cybers√©curit√©. 
        R√©ponds √† la question en te basant UNIQUEMENT sur le contexte fourni ci-dessous.
        Si la r√©ponse n'est pas dans le document, dis "Je ne sais pas".
        
        Contexte : {context}
        
        Question : {question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        
        # La cha√Æne de traitement (Chain)
        chain = (
            {"context": retriever, "question": lambda x: x}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # Affichage de la r√©ponse
        with st.spinner("L'IA r√©fl√©chit..."):
            response = chain.invoke(question)
            st.write("### R√©ponse :")
            st.write(response)